import os
import logging
from datetime import datetime
from enum import Enum
from typing import Optional
from dashboard import AgentDashboard
from fastapi import FastAPI
from langchain.llms import OpenAI
from langchain_anthropic import ChatAnthropic
from langchain_mistralai import ChatMistralAI
from langchain_groq import ChatGroq
from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='\033[36m%(asctime)s\033[0m | \033[32m%(levelname)s\033[0m | %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

app = FastAPI()
dashboard = AgentDashboard()

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

class LLMProvider(Enum):
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    MISTRAL = "mistral"
    GROQ = "groq"

def init_llm(provider: LLMProvider = LLMProvider.OPENAI) -> Optional[OpenAI]:
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π —Ä–∞–∑–Ω—ã—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    try:
        if provider == LLMProvider.OPENAI:
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                logger.warning("OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return None
            return OpenAI(
                temperature=0.7,
                model_name="gpt-4",
                openai_api_key=api_key,
                max_retries=3,
                request_timeout=30
            )
        elif provider == LLMProvider.ANTHROPIC:
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                logger.warning("ANTHROPIC_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return None
            return ChatAnthropic(
                model="claude-3-opus-20240229",
                temperature=0.7,
                anthropic_api_key=api_key,
                max_retries=3,
                timeout=30
            )
        elif provider == LLMProvider.MISTRAL:
            api_key = os.getenv("MISTRAL_API_KEY")
            if not api_key:
                logger.warning("MISTRAL_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return None
            return ChatMistralAI(
                model="mistral-large-latest",
                temperature=0.7,
                mistral_api_key=api_key,
                max_retries=3,
                timeout=30
            )
        elif provider == LLMProvider.GROQ:
            api_key = os.getenv("GROQ_API_KEY")
            if not api_key:
                logger.warning("GROQ_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
                return None
            return ChatGroq(
                model="mixtral-8x7b-32768",
                temperature=0.7,
                groq_api_key=api_key,
                max_retries=3,
                timeout=30
            )
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ {provider.value}: {str(e)}")
        return None

llm = init_llm()
memory = ConversationBufferMemory()
conversation = ConversationChain(llm=llm, memory=memory) if llm else None

@app.get("/health")
async def health_check():
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π healthcheck —Å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º –≤—Å–µ—Ö –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤"""
    providers_status = {
        "openai": bool(os.getenv("OPENAI_API_KEY")),
        "anthropic": bool(os.getenv("ANTHROPIC_API_KEY")),
        "mistral": bool(os.getenv("MISTRAL_API_KEY")),
        "groq": bool(os.getenv("GROQ_API_KEY"))
    }
    
    active_provider = os.getenv("LLM_PROVIDER", "openai")
    return {
        "status": "ok" if llm else "degraded",
        "llm_ready": bool(llm),
        "active_provider": active_provider,
        "providers_status": providers_status,
        "dependencies": {
            "openai": True,
            "anthropic": True,
            "mistral": True,
            "groq": True,
            "langchain": True,
            "fastapi": True
        },
        "sandbox": {
            "timeout_ms": int(os.getenv("E2B_TIMEOUT_MS", 300000)),
            "remaining_ms": "N/A"
        },
        "timestamp": datetime.now().isoformat()
    }

@app.on_event("shutdown")
async def shutdown_event():
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è —Ä–∞–±–æ—Ç—ã sandbox"""
    logger.info("–ò–Ω–∏—Ü–∏–∏—Ä–æ–≤–∞–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã sandbox")
    # –õ–æ–≥–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

# –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ò–ò-–∞–≥–µ–Ω—Ç–∞
SYSTEM_PROMPT = """–¢—ã - –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –°–ª–µ–¥—É–π –ø—Ä–∞–≤–∏–ª–∞–º:
1. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –∏ –ø–æ –¥–µ–ª—É
2. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —è–∑—ã–∫–∏
3. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –∫–æ–¥ —Å –æ—Ç—Å—Ç—É–ø–∞–º–∏
4. –ü—Ä–æ–≤–µ—Ä—è–π —Ñ–∞–∫—Ç—ã –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º
5. –†–∞–∑–¥–µ–ª—è–π —Å–ª–æ–∂–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã –Ω–∞ –ø—É–Ω–∫—Ç—ã

–ü—Ä–∏–º–µ—Ä —Ö–æ—Ä–æ—à–µ–≥–æ –æ—Ç–≤–µ—Ç–∞:
\"\"\"
–î–ª—è —Ä–µ—à–µ–Ω–∏—è –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏ –Ω–∞ Python:

1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
```bash
pip install requests pandas
```

2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —ç—Ç–æ—Ç –∫–æ–¥:
```python
import requests
response = requests.get('https://api.example.com/data')
print(response.json())
```

3. –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:
- –í–∞—Ä–∏–∞–Ω—Ç A: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å aiohttp –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ—Å—Ç–∏
- –í–∞—Ä–∏–∞–Ω—Ç B: –¥–æ–±–∞–≤–∏—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
\"\"\"
"""

@app.post("/chat")
async def chat(message: str):
    """–û—Å–Ω–æ–≤–Ω–æ–π endpoint –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∞–≥–µ–Ω—Ç–æ–º"""
    if not conversation:
        error_msg = "–°–µ—Ä–≤–∏—Å LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ API –∫–ª—é—á –∏ –ª–æ–≥–∏."
        logger.error(error_msg)
        return {"error": error_msg}, 503
        
    start_time = datetime.now()
    logger.info(f"\n{'='*40}\nüì© –í—Ö–æ–¥—è—â–∏–π –∑–∞–ø—Ä–æ—Å: {message}\n{'='*40}")
    
    try:
        # –õ–æ–≥–∏—Ä—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        logger.info(f"\nüìù –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç:\n{SYSTEM_PROMPT}\n{'-'*40}")
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –∫ —Å–æ–æ–±—â–µ–Ω–∏—é
        full_message = f"{SYSTEM_PROMPT}\n\n–í–æ–ø—Ä–æ—Å: {message}"
        logger.info(f"\nüí≠ –ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –ò–ò:\n{full_message}\n{'-'*40}")
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç –æ—Ç –ò–ò
        response = conversation.predict(input=full_message)
        logger.info(f"\nü§ñ –û—Ç–≤–µ—Ç –ò–ò (—Å—ã—Ä–æ–π):\n{response}\n{'-'*40}")
        
        # –û—á–∏—Å—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
        clean_response = response.replace(SYSTEM_PROMPT, "").strip()
        logger.info(f"\n‚ú® –û—á–∏—â–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç:\n{clean_response}\n{'-'*40}")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
        exec_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"‚è± –í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {exec_time:.2f} —Å–µ–∫\n{'='*40}\n")
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—à–±–æ—Ä–¥
        dashboard.update_stats(True, exec_time)
        return {
            "response": clean_response,
            "metadata": {
                "processing_time": exec_time,
                "model": "gpt-4" if llm else "none",
                "status": "success"
            }
        }
    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–∞: {str(e)}"
        logger.error(f"\n‚ùå –û—à–∏–±–∫–∞:\n{error_msg}\n–¢–∏–ø: {type(e).__name__}\n{'='*40}")
        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–∞—à–±–æ—Ä–¥ –ø—Ä–∏ –æ—à–∏–±–∫–µ
        dashboard.update_stats(False, 0)
        return {
            "error": error_msg,
            "type": type(e).__name__,
            "metadata": {
                "status": "error",
                "model": "gpt-4" if llm else "none"
            }
        }, 500

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)